{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0850bcfa",
   "metadata": {},
   "source": [
    "# Curso Profesional de Scikt-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec555ba9",
   "metadata": {},
   "source": [
    "Scikit-learn comienza su historia en 2007 por David Cournapeau como un proyecto dentro del Google Summer of Code. Matthieu Brucher continúa como parte de su tesis.\n",
    "\n",
    "- La versión estable actual es la v0.22.\n",
    "- Curva de aprendizaje suave.\n",
    "- Es una libreria muy versátil.\n",
    "- Comunidad de soporte.\n",
    "- Usado en producción.\n",
    "- Integración con librerias externas.\n",
    "\n",
    "\n",
    "## Módulos de Sckit-learn\n",
    "\n",
    "1. Clasificación\n",
    "2. Regresión\n",
    "3. Clustering\n",
    "4. Preprocesamiento\n",
    "5. Reducción de la dimensionalidad\n",
    "6. Selección del modelo\n",
    "\n",
    "## Tipos de Aprendizaje de maquina\n",
    "1. **Aprendizaje Supervisado (por observación)**: De nuestros datos se puede inferir directamente.\n",
    "2. **Aprendizaje por Refuerzo (prueba y error)**: No tenemos información precisa sobre lo que esperamos pero sí podemos evaluar si una decisión tomada por la máquina es buena o mala.\n",
    "3. **Aprendizaje No Supervisado (por descubrimiento)**: No sabemos qué esperar de nuestros datos y queremos explorar la estructura o las relaciones del dataset.\n",
    "\n",
    "*Nota:* El ML es solamente una de las posibles ramas que tiene la inteligencia artificial, para otro tipos de problemas existen:\n",
    "- Algoritmos evolutivos.\n",
    "- Lógica difusa.\n",
    "- Agentes.\n",
    "- Sistemas expertos.\n",
    "\n",
    "## Limitaciones de Sklearn\n",
    "1. No es una herramienta de Computer Vision (imagenes).\n",
    "2. No se puede correr en GPUs, solo en CPU (las tareas pueden tomar más tiempo).\n",
    "3. No es una herramienta de estadística avanzada, un alternativa podría ser SciPy.\n",
    "4. No es muy flexible en temas de Deep Learning.\n",
    "\n",
    "## Problema de Clasificación\n",
    "La variable de salida se puede definir en ciertas categorias bien definidas y excluyentes entre sí.\n",
    "\n",
    "## Problema de Regresión\n",
    "La variable de salida es continua y se requiere modelar el comportamiento dadas otras variables correlacionadas.\n",
    "\n",
    "## Problema de Clustering\n",
    "Queremos descubrir subconjuntos de datos similares dentro del dataset o queremos encontrar valores que se salen del comportamiento global.\n",
    "\n",
    "## Fundamentos matematicos que se requieren\n",
    "1. Funciones y trigonometría\n",
    "2. Algebra lineal\n",
    "3. Optimización de funciones (maximos y minimos)\n",
    "4. Calculo basico (derivas e integrales)\n",
    "5. Probabilidad básica\n",
    "6. Combinaciones y permutaciones\n",
    "7. Variables aletorias y sus distribuciones\n",
    "8. Teorema de Bayes\n",
    "9. Pruebas de hipotesis e intervalos de confianza\n",
    "\n",
    "\n",
    "### Datasets que usaremos en el curso:\n",
    "\n",
    "- **World Happiness Report**: Es un dataset que desde el 2012 recolecta variables sobre diferentes países y las relaciona con el nivel de felicidad de sus habitantes. *Nota: Este data set lo vamos a utilizar para temas de regresiones.*\n",
    "\n",
    "- **The Ultimate Halloween Candy Power Ranking**: Es un estudio online de 269 mil votos de más de 8371 IPs deferentes. Para 85 tipos de dulces diferentes se evaluaron tanto características del dulce como la opinión y satisfacción para generar comparaciones. \n",
    "*Nota: Este dataset lo vamos a utilizar para temas de clustering.*\n",
    "\n",
    "- **Heart disease prediction**: Es un subconjunto de variables de un estudio que realizado en 1988 en diferentes regiones del planeta para predecir el riesgo a sufrir una enfermedad relacionada con el corazón. \n",
    "*Nota: Este data set lo vamos a utilizar para temas de clasificación.*\n",
    "\n",
    "## ¿Cómo afectan nuestros features a los modelos de Machine Learning?\n",
    "\n",
    "¿Qué son los features? Son los atributos de nuestro modelo que usamos para realizar una interferencia o predicción. Son las variables de entrada.\n",
    "\n",
    "*Más features simpre es mejor, ¿verdad? La respuesta corta es: NO*. En realidad si tenemos variables que son irrelevantes pasarán estas cosas:\n",
    "\n",
    "- Se le abrirá el paso al ruido.\n",
    "- Aumentará el costo computacional.\n",
    "- Si introducimos demasiados features y estos tienen valores faltantes, se harán **sesgos** muy significativos y vamos a perder esa capacidad de predicción. \n",
    "\n",
    "*Nota: Hacer una buena selección de nuestro features, hará que nuestros algoritmos corran de una manera mas eficiente.*\n",
    "\n",
    "**Algo que debemos que recordar es que nuestro modelo de ML puede caer en uno de 2 escenarios que debemos evitar:**\n",
    "\n",
    "* **El Underfitting:** Significa que nuestro modelo es demasiado simple, en donde nuestro modelo no está captando los features y nuestra variable de salida, por lo cual debemos de investigar variables con mas significado o combinaciones o transformaciones para poder llegar a nuestra variable de salida.\n",
    "\n",
    "* **El Overfitting**: Significa que nuestro modelo es demasiado complejo y nuestro algoritmo va a intentar ajustarse a los datos que tenemos, pero no se va a comportar bien con los datos del mundo real. Si tenemos overfiting lo mejor es intentar seleccionar los features de una manera mas critica descartando aquellos que no aporten información o combinando algunos quedándonos con la información que verdaderamente importa.\n",
    "\n",
    "**¿Qué podemos hacer para solucionar estos problemas?**\n",
    "\n",
    "Aplicar técnicas reducción de la dimensionalidad. Utilizaremos el algoritmo de PCA.\n",
    "Aplicar la técnica de la regulación, que consiste en penalizar aquellos features que no le estén aportando o que le estén restando información a nuestro modelo.\n",
    "Balanceo: Se utilizará Oversampling y Undersampling en problemas de rendimiento donde tengamos un conjunto de datos que está desbalanceado, por ejemplo en un problema de clasificación donde tenemos muchos ejemplos de una categoría y muy pocos de otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ab5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
